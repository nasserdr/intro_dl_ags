{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd with tensors"
      ],
      "metadata": {
        "id": "K-BUF2iqY_0n",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "id": "nbCTeu9AZDak",
        "colab_type": "code",
        "colab": {},
        "gather": {
          "logged": 1665406112006
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(4,3, requires_grad=True)\r\n",
        "w"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "tensor([[ 0.1919,  1.2638, -1.2904],\n        [-0.7911, -0.0209, -0.7185],\n        [ 0.5186, -1.3125,  0.1920],\n        [ 0.5428, -2.2188,  0.2590]], requires_grad=True)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "id": "AIpMXYEQZDYG",
        "colab_type": "code",
        "colab": {},
        "gather": {
          "logged": 1665406115789
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w.requires_grad_(False) #The _ is for in place operations. It's quite conventionally used all over pytorch\r\n",
        "w"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "tensor([[ 0.1919,  1.2638, -1.2904],\n        [-0.7911, -0.0209, -0.7185],\n        [ 0.5186, -1.3125,  0.1920],\n        [ 0.5428, -2.2188,  0.2590]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "id": "tPCDpREGZDV_",
        "colab_type": "code",
        "colab": {},
        "gather": {
          "logged": 1665406124855
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w.requires_grad_(True)\r\n",
        "w"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "tensor([[ 0.1919,  1.2638, -1.2904],\n        [-0.7911, -0.0209, -0.7185],\n        [ 0.5186, -1.3125,  0.1920],\n        [ 0.5428, -2.2188,  0.2590]], requires_grad=True)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {
        "id": "tc6j07MfZDTW",
        "colab_type": "code",
        "colab": {},
        "gather": {
          "logged": 1665406128173
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.exp(w)\r\n",
        "\r\n",
        "''' Notice that pytorch keeps track of the gradient operation. In this case, since the tensor is programmed as differentiable, pytorch\r\n",
        "stick a gradient function with it. In this case, it's the exponential backward function.\r\n",
        "'''\r\n",
        "y \r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "tensor([[1.2115, 3.5388, 0.2752],\n        [0.4533, 0.9793, 0.4875],\n        [1.6797, 0.2691, 1.2117],\n        [1.7208, 0.1087, 1.2956]], grad_fn=<ExpBackward0>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "id": "FYs5XvOlZDOH",
        "colab_type": "code",
        "colab": {},
        "gather": {
          "logged": 1665406131775
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Try computing the y.backward(). The backward function works only on scalar. There is one additional requirement to make it work on vectors:\r\n",
        "# Check this link: https://abishekbashyall.medium.com/playing-with-backward-method-in-pytorch-bd34b58745a0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.grad_fn)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<ExpBackward0 object at 0x7f1bbc07dc40>\n"
        }
      ],
      "execution_count": 40,
      "metadata": {
        "id": "sH5UvYQRZDLx",
        "colab_type": "code",
        "colab": {},
        "gather": {
          "logged": 1665406365748
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's do the same with a scalar\r\n",
        "output = y.mean()\r\n",
        "output"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/plain": "tensor(1.1026, grad_fn=<MeanBackward0>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "id": "5ZLCejs4ZDJY",
        "colab_type": "code",
        "colab": {},
        "gather": {
          "logged": 1665406368887
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's get the gradients of w\r\n",
        "print(w.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "None\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "id": "ZadWbpkSZDHG",
        "colab_type": "code",
        "colab": {},
        "gather": {
          "logged": 1665406376810
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#With the backward function, the gradients are calculated\r\n",
        "output.backward()"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665406388779
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Notice that we did not get None when printing the gradients of w. We rather got number which are the gradients of w\r\n",
        "print(w.grad)\r\n",
        "\r\n",
        "# From w, we compute the exponentials, which gives y. Then, we have an average over the exp, which gives output."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[0.1010, 0.2949, 0.0229],\n        [0.0378, 0.0816, 0.0406],\n        [0.1400, 0.0224, 0.1010],\n        [0.1434, 0.0091, 0.1080]])\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665406390756
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.requires_grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "True\n"
        }
      ],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665406401907
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\r\n",
        "    output = (w + y).mean()\r\n",
        "print(output.requires_grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "False\n"
        }
      ],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665406404779
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Using Autograd.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}